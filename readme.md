# Skill discovery in reinforcement learning based on the diverse density algorithm by Amy McGovern

**Title of your Research project:** Skill discovery in reinforcement learning based on the diverse density algorithm by Amy McGovern

**Brief description of your Research project:** Approximation of the diverse density algorithm with the convolutional neural network, which would instantly generate a density map, given an original map of the environment, that, in turn, would be used before the agent interacts with the environment, to identify subgoals.

**Generated diverse density maps as the agent interacted with the gridworld, where black indicate the densest and therefore most important states:**

![](https://lh6.googleusercontent.com/_livLgPgh3Or27-21iU0kSnNylnAsDFKVrkBUVclZzXZ-U358fiOfXjNNQBzr9mn70bEoqOkF-YET98FRacyEZ5pk0tZ986AHQU4lTxsbjDwXMah8vj6n5j0a1LFlNyrOSUw13_k1FlEQyCHUTLApsc)
**![](https://lh3.googleusercontent.com/0zOiPJxP4tE9CG0lEnW2JLHDkwZ5DX6Q4h7pYcG3MJX5rCpdyroc22aKluU46_cP6yEzMFQ9rbDt2AQQcLUqKjK-nrRraVljEofVPxtv1FIl4zw5PZzFG_mu7ryjvkAfCxvJ87AkEN_7RGYM23XdpPE)**
**![](https://lh5.googleusercontent.com/poM3bEP_SCvaDn9WI5mI4JGWD3VnZsaG_W5MuSm08E4Z5kIRntqQVBb9Zxm0eZ6HrWbVvSjO3QzKEYCpjb_7l2txIgbTkS8ahwQv1IDgG0aj_mxCEvYe0Q6TwuYq8Gtkl3CBir3JeTBQ3ALfi2dKiuo)**

**Using the neural network to produce the diverse density maps:** I compared how the maps were generated with a plain dense neural network and a neural network with the convolutional layers, and my results showed that both the training loss and value loss were decreased once the convolutional layers were added. The ReLU and Sigmoid activation functions were the most successful ones.

**For the testing I used 500 iterations for both models:**

**![](https://lh4.googleusercontent.com/s0VnyqKSFJXqdos8atGZDzHFMEcmmReOZqFhiLlS-_PBPFkcHdRu1SHxfqXv0moo_1OcXfCR5ZPcRtkzXM2Io_vgFwowRGshipxuu8lBNleEIguz7cQ_7mc04v8mjStLXS0Fi2_PB7WhaohsUFem8jU)**
**![](https://lh5.googleusercontent.com/h6f2PZODJkEuEZOYpQghylvVt2DHft78FlTLOPAc65ZYwf8ai7ls1kwigaNlkb2KMzjB9HB-dNd3GIXUQCeZgcVITeKoIZUhLMcz5dp7AI_W2rRtZlgdZYCRJGq-Z0GVvbJ8NGQTWiHGdCQX1oNCRvQ)**
**![](https://lh6.googleusercontent.com/ETtHuvp5El8uXtWJKkpKiusPm5d_N3VD9kua6-9IY62J4J0GYDrCqlSIymvaq3VfySpoirCgffSY4qfNRQ6YuQnV2Ra3VIckUPaeT3QQoe5TSJnCS0Yf1JxsuINW7UVL53lWuPZNPwbLkZIXyALbVmY)**
In my case I kept the density of the initial state and a final goal state for a showcase, but it would be a taboo state when generating the options in the algorithm.

**Remarks:** the more complex the gridworld environment is, the more training data is required for the models, since they were not written to have a complex algorithm that focuses on the low-level features. So, for example, since I did not have any horizontal walls in my training dataset, the bottlenecks in those walls would not be detected as the subgoals.

**On the graphs below we can clearly see that only the subgoals around the vertical walls were identified correctly.**

**![](https://lh4.googleusercontent.com/QFykTFJbWTaZ-vjUzBAGGQq9Freh8FJuIDt1P41Lfln3ghKJ4Yc4sQD1tHLKbQXQdz_PjCQs82FrYaY-LF47onCNNmoR29q5uNm-RPx7ISjXgwQ8P_YBv7t75zOt5668QZwaJ7VUyRdAzEq8ULmKEpQ)**
**Generating options:** all the hierarchical actions were generated using the experience replay, which utilises the past trajectories and a goal in order to create option’s q-table and policy.

1)  Case when diverse density maps are being developed and changed as an agent explores the environment: every 5 iterations a new option is generated, and it takes some time in order to find the good subgoals as the candidate for the options, since the first visit may be randomly an unimportant state, in the beginning of learning.

2)  Case when diverse density maps were generated by a neural network beforehand: important states are optimally identified just when the agent begins to learn about the environment, therefore it can take advantage of the learnt important states, after getting some trajectories. In my algorithm, I erase the old options every n iterations (5 for testing), but keep the subgoals the same, which helps me to create all the appropriate options every n-th iteration, instead of producing them one by one with a certain step.

As seen from the graphs, when the dd map is generated before the agent interacts with the environment, learning is accelerated by around 8 steps in comparison to the options with the developing dd map, and by more than 52 steps, compared to primitive actions.

**![](https://lh3.googleusercontent.com/rsn2oofuFHpui7oKjX73-jP7pm7zbx1T1Ryu9tPXeiAXI_ESkWftAWqAI_coyHlmNTAu3GSurRW4SvRCgQXQcSdiSAP3Qe4Mo9NwM8Y86y9vkYX4gkloTD3TsU8igLQnfmFCVDT8IwKmEJ2W_kKNUlU)**
**Future development:** Another way to accelerate the learning even more, having the diverse density map, would be to give states in the q-table values based on the diverse density values. So, for example if we have the following state in the centre as: , then for this state, going left and right can be around 0.8, while going up and down around 0. In this case we exclude up and down, but the difficulty is caused by determining whether right or left is optimal.

**How the options transform the diverse density map in fewer iterations, an observation (discovering options with the developing dd map):**

First example:

**![](https://lh3.googleusercontent.com/XAjQtFwsGkXoeVG_UOyqhQs6tbxC8TmRCCIXQreQjgGBVVM4snjiuHd1dvkqNn8a1BJJrZp2L4gUlmmuBOSHnVmAhNf6ToWtuhCMX2VmobXxvFlOZjRBMRzbYzf7cu_C4aIluEh4Bent9wLOxmWpjRQ)**
**![](https://lh6.googleusercontent.com/QA9yi9Mm3qbiAmndWMAYlxscXwj9WQTnuqKz_d3wxyEasP4sRBFMDO5YL_3-BqgAXJ9TwmtWEKmoqX4i7gP1I1CU3fqZQvsQGyc2toZWhf8IiYgjLEeBFDqI5OZ1mdZ7zyVEUN9SLjrLqiCoGfeC6Zs)**
**![](https://lh5.googleusercontent.com/QheSzGNVyjQmL1xcdZNFN3XOOzNSmCKLMLSglaJRdlBMIxpyCj-69iogsWcn-T52M8CigIce4qKhhSDX3CDRtRIXrbUsfHBW0yN1___5HkKaxQLCHTeFP4lAMum2l-kEIzJRyqAVLKaEn_qknueNYEg)**
Between each map there is a difference in 5 iterations, therefore we can observe how fast a good option, found after the state on the second image, transformed the diverse density map

Second example:

**![](https://lh6.googleusercontent.com/kEzo-5e3Jl_rO-N-0cxR6PUm0lLRyHK0RM1nPxLYY0xDyHu7x0HEIGTg_53D59-HdSvQt3c-U12DYnuqAGAcGEujT9_vGd2y__Fr7tmy5pzKN_p1yX3FBOnfJfOMbVWb2QVKjnx63VV6D2M-59IbSMQ)**
**![](https://lh3.googleusercontent.com/as1M-L6w2KQxnX0ltJMR3oZnULOz7zFmNE3cgyECKiM0JMi5WVQqhq-o5qJ1ElOyKLMtmRuGOVYj60DV8HYPkfT1--zRWZmpUIS1ah0-lTlgwjWIahJk5gmWGLX25owy1bUd2ur4BdQBoCNonA7Ujl0)**
